---
title:   
layout: single
permalink: /projects/
author_profile: true
---
- ## Robust Explainability in Multimodal Models (CLIP)
- Proposed a novel adversarial attack targeting **patch-level similarity maps**.
- Reduced explanation faithfulness while **preserving model predictions**.
- Designed dual-path self-attention + selective patch masking defenses.
- Evaluated on Talk2Car, Flickr30k, COCO, ImageNet-1K.

**Tools:** Python, PyTorch, Vision Transformers, COCO API, XAI libraries.

---
- ## Meta-Learning Framework for Sponge Attack Detection
- Built an **RNN-based meta-learning detector** for sponge attacks.
- Achieved **98% accuracy** on CIFAR-10, MNIST, GTSRB, and TinyImageNet.
- Handles training dynamics, LSTM time-series patterns, meta-learned gradients.

**Tools:** PyTorch, Meta-Learning (MAML), LSTM.

---

- ## Ensemble Learning for Adversarial Detection

- Designed ensemble-based IDS for ICS + CV adversarial attacks.
- Achieved **99% detection accuracy** on sensor and MNIST datasets.
- Integrated TensorFlow, ART, and Scikit-Learn.

---

- ## Moving Target Defense for CPS Security

- Developed a CPS defense for deception attack detection.
- Accurate against replay, false-data, and covert attacks.
- Improved reliability of real-time control systems.

